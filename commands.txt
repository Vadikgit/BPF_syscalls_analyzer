make map_extractor
make analyzer
make read_stat
make read_map

sudo ./analyzer -start
sudo ./read_map 1000 10
sudo ./analyser -stop

sudo bpftool map

sudo ./analyzer -start -pid 5511

sudo cat /sys/kernel/debug/tracing/trace_pipe


docker run --rm --name linux -it ubuntu:18.04 /bin/bash


docker top naughty_mccarthy

sudo apt-get install iputils-ping

apt-get update

pstree -p

docker exec -ti linux sh

apt-get install psmisc


from proc:
 cat 6141/status | grep -i NSpid

uname -r

stat --printf='%d %i' /proc/self/ns/pid
stat /proc/self/ns/pid


docker run --name linux -it --rm ubuntu:latest /bin/bash
docker run -v shared_vol:/home/ubuntu/vol --name linux -it --rm ubuntu:18.04 /bin/bash
docker run -v "/home/vadim/Документы/prog/bpf_process_tree_builder/shared_vol:/home/ubuntu/vol" --name linux -it --rm ubuntu:latest /bin/bash
docker run --name py_app -it --rm vadksen01/python_app
docker inspect -f '{{.State.Pid}}' <container id>

pstree -ps -U 12655



sudo ./tree_extractor 6 7 8

5816
sudo bpftool map update name to_track_pid key 0 0 0 0 value 184 22 0 0
sudo bpftool map dump name SeqNums

printf "%x\n" 5816
16b8

6737
sudo bpftool map update name to_track_pid key 0 0 0 0 value hex 51 1a 0 0


source script.sh
recProcedure 0


sudo ./read_map 17 "1 2 3 4" 930 20

bash -c "source script.sh; ls; ping ya.ru -c 1; recProcedure $i; exit"
bash -c "source script.sh; ls; ps; recProcedure $i; exit"



/usr/bin/time --format "%U %e %S" dd if=/dev/sda3 of=/dev/null bs=4000B count=2000000
time dd if=/dev/sda of=/dev/null bs=4096B count=800000

mapfile -t my_array < <( { /usr/bin/time --format "%U\n%e\n%S\n" dd if=/dev/sda3 of=/dev/null bs=4000B count=100000 status=none; } 2>&1 )





docker pull apache/kafka:3.8.0
docker run -p 9092:9092 apache/kafka:3.8.0
docker ps

docker exec -it -w /opt/kafka/bin funny_satoshi sh


./kafka-topics.sh --create --topic test --bootstrap-server 127.0.0.1:9092

./kafka-topics.sh --delete --topic test --bootstrap-server localhost:9092


./kafka-console-consumer.sh --topic test --bootstrap-server 127.0.0.1:9092 \
--from-beginning \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"

./kafka-console-consumer.sh --topic test --bootstrap-server 127.0.0.1:9092 \

--property print.key=true \
--property print.value=true \
--property key.separator=" : " 

./kafka-console-consumer.sh --topic test --bootstrap-server 127.0.0.1:9092 \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"

./kafka-console-consumer.sh --topic test --bootstrap-server 127.0.0.1:9092 \
--from-beginning \
--property print.key=true \
--property print.value=false \
--key-deserializer "org.apache.kafka.common.serialization.BytesDeserializer"


./kafka-console-producer.sh --topic user-statuses-topic --bootstrap-server 127.0.0.1:9092

./kafka-console-consumer.sh --topic user-statuses-topic --from-beginning --bootstrap-server 127.0.0.1:9092


./kafka-console-consumer.sh --topic pulse --bootstrap-server 127.0.0.1:9092 \
--from-beginning \
--value-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"


./kafka-console-consumer.sh --topic pulse --bootstrap-server 127.0.0.1:9092 \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer" \
--value-deserializer "org.apache.kafka.common.serialization.IntegerDeserializer"

./kafka-console-consumer.sh --topic pulse_for_10min --bootstrap-server 127.0.0.1:9092 \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer" \
--value-deserializer "org.apache.kafka.common.serialization.DoubleDeserializer"


./kafka-console-consumer.sh --topic location --bootstrap-server 127.0.0.1:9092 \
--from-beginning \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer" \
--value-deserializer "org.apache.kafka.common.serialization.BytesDeserializer"


./kafka-console-consumer.sh --topic average_speed_vector --bootstrap-server 127.0.0.1:9092 \
--from-beginning \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer" \
--value-deserializer "org.apache.kafka.common.serialization.BytesDeserializer"



./kafka-console-consumer.sh --topic average_speed_vector --bootstrap-server 127.0.0.1:9092 \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer"



./kafka-console-consumer.sh --topic motion_point --bootstrap-server 127.0.0.1:9092 \
--property print.key=true \
--property print.value=true \
--property key.separator=" : " \
--key-deserializer "org.apache.kafka.common.serialization.LongDeserializer"


./kafka-console-consumer.sh --topic most_filled_sectors --bootstrap-server 127.0.0.1:9092

./kafka-console-producer.sh  --topic user-statuses-topic --bootstrap-server 127.0.0.1:9092


/home/vadim/python3.7/bin/python tcp_server_with_kafka_producer.py


flink:
cd flink/flink-2.0-preview1

./bin/start-cluster.sh

./bin/flink run examples/streaming/WordCount.jar --input "/home/vadim/stream_data_analysis/flink/input.txt" --output "/home/vadim/stream_data_analysis/flink/output.txt"

./flink run /home/vadim/stream_data_analysis/flink/untitled/trying2/build/libs/trying2-0.1-SNAPSHOT-all.jar

scp -i /home/vadim/Документы/prog/key1 install-docker.sh admin@<IP>:/home/admin


clear; pstree -p | grep map_extractor

docker run --rm -p 15672:15672 -p 5672:5672 rabbitmq:3.10.7-management
docker run --rm -p 15672:15672 -p 5672:5672 rabbitmq:4.0-management


g++ -g -Wall rabbit.cpp -o main -lamqpcpp -lpthread -ldl -lev


apt install -y git-all
apt install -y clang libelf1 libelf-dev zlib1g-dev
apt install -y llvm
apt install -y build-essential
apt install -y openssl
apt install -y libssl-dev
apt install -y libev-dev

git clone https://github.com/CopernicaMarketingSoftware/AMQP-CPP.git
cd AMQP-CPP
make
make install
cd ..

export RABBITMQ_SERVER_ADDRESS=<IP>



docker build -t cassandra1 .

docker run --rm -d --name cassandra --hostname cassandra -p 9042:9042 cassandra

docker stop cassandra

cqlsh

CREATE KEYSPACE IF NOT EXISTS test_keyspace WITH REPLICATION =
{ 'class' : 'SimpleStrategy',
'replication_factor' : '1'
};

CREATE TABLE IF NOT EXISTS test_keyspace.test_data (
    id int PRIMARY KEY,
    name text
);

INSERT INTO test_keyspace.test_data (id, name) VALUES (43, 'Vadim');

../../venv/bin/python insert.py

CREATE KEYSPACE IF NOT EXISTS ks1 WITH REPLICATION =
{ 'class' : 'SimpleStrategy',
'replication_factor' : '1'
};

CREATE TABLE IF NOT EXISTS ks1.syscalls (
    global_id bigint PRIMARY KEY,
    hostname text,
    per_node_per_analyzer_run_id bigint,
    syscall_number smallint,
    PID int,
    TGID int,
    UID int,
    GID int,
    core_id smallint,
    returned_value bigint,
    enter_date date,
    enter_time time,
    exec_duration_in_ns bigint
);


xxd data.clickhouse
xxd -i data.clickhouse



sudo apt-get install -y apt-transport-https ca-certificates curl gnupg
curl -fsSL 'https://packages.clickhouse.com/rpm/lts/repodata/repomd.xml.key' | sudo gpg --dearmor -o /usr/share/keyrings/clickhouse-keyring.gpg

ARCH=$(dpkg --print-architecture)
echo "deb [signed-by=/usr/share/keyrings/clickhouse-keyring.gpg arch=${ARCH}] https://packages.clickhouse.com/deb stable main" | sudo tee /etc/apt/sources.list.d/clickhouse.list
sudo apt-get update

sudo apt-get install clickhouse-server clickhouse-client


clickhouse-server
sudo service clickhouse-server start
sudo service clickhouse-server restart
sudo service clickhouse-server status

clickhouse-client

sudo tail -f /var/log/clickhouse-server/clickhouse-server.err.log


SELECT * FROM my_database.my_table INTO OUTFILE 'data.clickhouse' FORMAT Native


156
1|0011100 0|0000001

       9c 01



CREATE DATABASE my_database;

CREATE TABLE my_database.my_table (
    id UInt32,
    name String,
    age UInt32,
) ENGINE = MergeTree
ORDER BY id


CREATE TABLE my_database.my_table2 (
    id UInt32,
    name String,
    age UInt32,
) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = '...:5672',
                            rabbitmq_exchange_name = 'clickhouse-exchange',
                            rabbitmq_routing_key_list = 'myqueue',
                            rabbitmq_format = 'Native',
                            rabbitmq_exchange_type = 'fanout',
                            rabbitmq_num_consumers = 1,
                            rabbitmq_username = 'guest',
                            rabbitmq_password = 'guest',
                            date_time_input_format = 'best_effort';


                            rabbitmq_queue_consume = true;

CREATE TABLE my_database.my_table3 (
    id UInt32,
    name String,
    age UInt32,
) ENGINE = MergeTree
ORDER BY id


CREATE MATERIALIZED VIEW IF NOT EXISTS my_database.event_view1
TO my_database.my_table3 AS
SELECT
    id AS id,
    name AS name,
    age AS age
FROM my_database.my_table2;

INSERT INTO my_database.my_table (id, name, age) VALUES (1, 'Alice', 24), (2, 'Bob', 12), (3, 'Alex', 34), (4, 'John', 23);
INSERT INTO my_database.my_table (id, name, age) VALUES (5, 'ABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZABCDEFGHIJKLMNOPQRSTUVWXYZ', 32);
//////////////////////////////

CREATE TABLE IF NOT EXISTS my_database.syscalls (
    hostname String,
    per_node_per_analyzer_run_id UInt64,
    syscall_number UInt16,
    PID UInt32,
    TGID UInt32,
    UID UInt32,
    GID UInt32,
    core_id UInt16,
    returned_value UInt64,
    enter_date_time DateTime64(9, 'Europe/Moscow'),
    exit_date_time DateTime64(9, 'Europe/Moscow'),
) ENGINE = MergeTree
ORDER BY enter_date_time


CREATE TABLE my_database.syscalls_from_rabbitmq (
    c1 String,
    c2 UInt64,
    c3 UInt16,
    c4 UInt32,
    c5 UInt32,
    c6 UInt32,
    c7 UInt32,
    c8 UInt16,
    c9 UInt64,
    c10 UInt64,
    c11 UInt64,
) ENGINE = RabbitMQ SETTINGS rabbitmq_host_port = '...:5672',
                            rabbitmq_exchange_name = 'clickhouse-exchange',
                            rabbitmq_routing_key_list = 'myqueue',
                            rabbitmq_format = 'Native',
                            rabbitmq_exchange_type = 'fanout',
                            rabbitmq_num_consumers = 1,
                            rabbitmq_username = 'guest',
                            rabbitmq_password = 'guest',
                            date_time_input_format = 'best_effort';

CREATE MATERIALIZED VIEW IF NOT EXISTS my_database.event_view
TO my_database.syscalls AS
SELECT
    c1 AS hostname,
    c2 AS per_node_per_analyzer_run_id,
    c3 AS syscall_number,
    c4 AS PID,
    c5 AS TGID,
    c6 AS UID,
    c7 AS GID,
    c8 AS core_id,
    c9 AS returned_value,
    addNanoseconds(toDateTime64(0, 9, 'Europe/Moscow'), c10) AS enter_date_time,
    addNanoseconds(toDateTime64(0, 9, 'Europe/Moscow'), c11) AS exit_date_time
FROM my_database.syscalls_from_rabbitmq;

SELECT * FROM my_database.syscalls

TRUNCATE TABLE my_database.syscalls
SELECT max(per_node_per_analyzer_run_id) FROM my_database.syscalls

SELECT addNanoseconds(toDateTime64(0, 9, 'Europe/Moscow'), 1576800000000000000)

clickhouse-client < set_up_clickhouse.sql
